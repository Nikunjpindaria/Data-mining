{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1422b500",
   "metadata": {},
   "source": [
    "# Apriori Algorithm Implementation Assignment\n",
    "\n",
    "### Objective:\n",
    "You will implement the **Apriori algorithm** from scratch (i.e., without using any libraries like `mlxtend`) to find frequent itemsets and generate association rules.\n",
    "\n",
    "### Dataset:\n",
    "Use the [Online Retail Dataset](https://www.kaggle.com/datasets/vijayuv/onlineretail) from Kaggle. You can filter it for a specific country (e.g., `United Kingdom`) and time range to reduce size if needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85128a0",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "- Load the dataset\n",
    "- Remove rows with missing values\n",
    "- Filter out rows where `Quantity <= 0`\n",
    "- Convert Data into Basket Format\n",
    "\n",
    "ðŸ‘‰ **Implement code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows: 354345\n",
      "Unique items: 3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikunj\\AppData\\Local\\Temp\\ipykernel_3584\\3736625478.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('OnlineRetail.csv',encoding = 'ISO-8859-1' )\n",
    "# Preprocess as per the instructions above | We have already done in TASK 2\n",
    "df = df.dropna()\n",
    "df = df[(df.Quantity > 0) & (df.Country == 'United Kingdom')]\n",
    "df['Description'] = df['Description'].str.strip().str.lower()\n",
    "print(f\"Filtered rows: {len(df)}\")\n",
    "print(f\"Unique items: {df['Description'].nunique()}\")\n",
    "# Your Code Here\n",
    "# for Basket\n",
    "basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
    "basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
    "transactions = basket.apply(lambda row: set(row[row > 0].index), axis=1).tolist()\n",
    "print(f\"Total transactions: {len(transactions)}\")\n",
    "print(f\"Sample transaction: {transactions[0] if transactions else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c1947-0d94-48ae-a8fc-24a36a4811b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a37baf6",
   "metadata": {},
   "source": [
    "## Step 2: Implement Apriori Algorithm\n",
    "Step-by-Step Procedure:\n",
    "1. Generate Frequent 1-Itemsets\n",
    "Count the frequency (support) of each individual item in the dataset.\n",
    "Keep only those with support â‰¥ min_support.\n",
    "â†’ Result is L1 (frequent 1-itemsets)\n",
    "2. Iterative Candidate Generation (k = 2 to n)\n",
    "While L(k-1) is not empty:\n",
    "a. Candidate Generation\n",
    "\n",
    "Generate candidate itemsets Ck of size k from L(k-1) using the Apriori property:\n",
    "Any (k-itemset) is only frequent if all of its (kâˆ’1)-subsets are frequent.\n",
    "b. Prune Candidates\n",
    "Eliminate candidates that have any (kâˆ’1)-subset not in L(k-1).\n",
    "c. Count Support\n",
    "For each transaction, count how many times each candidate in Ck appears.\n",
    "d. Generate Frequent Itemsets\n",
    "Form Lk by keeping candidates from Ck that meet the min_support.\n",
    "Repeat until Lk becomes empty.\n",
    "Implement the following functions:\n",
    "1. `get_frequent_itemsets(transactions, min_support)` - Returns frequent itemsets and their support\n",
    "2. `generate_candidates(prev_frequent_itemsets, k)` - Generates candidate itemsets of length `k`\n",
    "3. `calculate_support(transactions, candidates)` - Calculates the support count for each candidate\n",
    "\n",
    "**Write reusable functions** for each part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement apriori functions below\n",
    "\n",
    "def get_frequent_itemsets(transactions, min_support):\n",
    "    frequent_itemsets = {}\n",
    "    item_counts = {}\n",
    "\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_counts[frozenset([item])] = item_counts.get(frozenset([item]), 0) + 1\n",
    "\n",
    "    num_transactions = len(transactions)\n",
    "    L1 = {itemset: count / num_transactions for itemset, count in item_counts.items() if (count / num_transactions) >= min_support}\n",
    "    frequent_itemsets.update(L1)\n",
    "\n",
    "    Lk_minus_1 = L1\n",
    "    k = 2\n",
    "\n",
    "    while Lk_minus_1:\n",
    "        Ck = generate_candidates(Lk_minus_1.keys(), k)\n",
    "        support_counts = calculate_support(transactions, Ck)\n",
    "        Lk = {itemset: count / num_transactions for itemset, count in support_counts.items() if (count / num_transactions) >= min_support}\n",
    "\n",
    "        if Lk:\n",
    "            frequent_itemsets.update(Lk)\n",
    "            Lk_minus_1 = Lk\n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "def generate_candidates(prev_frequent_itemsets, k):\n",
    "    candidates = set()\n",
    "    list_prev_itemsets = list(prev_frequent_itemsets)\n",
    "\n",
    "    for i in range(len(list_prev_itemsets)):\n",
    "        for j in range(i + 1, len(list_prev_itemsets)):\n",
    "            itemset1 = list_prev_itemsets[i]\n",
    "            itemset2 = list_prev_itemsets[j]\n",
    "\n",
    "            if k > 2:\n",
    "                sorted_itemset1 = sorted(list(itemset1))\n",
    "                sorted_itemset2 = sorted(list(itemset2))\n",
    "                if sorted_itemset1[:-1] == sorted_itemset2[:-1]:\n",
    "                    new_itemset = frozenset(itemset1.union(itemset2))\n",
    "                    is_valid = all(frozenset(subset) in prev_frequent_itemsets for subset in combinations(new_itemset, k - 1))\n",
    "                    if is_valid:\n",
    "                        candidates.add(new_itemset)\n",
    "            else:\n",
    "                new_itemset = itemset1.union(itemset2)z\n",
    "                if len(new_itemset) == k:\n",
    "                    candidates.add(new_itemset)\n",
    "\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def calculate_support(transactions, candidates):\n",
    "    support_counts = {candidate: 0 for candidate in candidates}\n",
    "    for transaction in transactions:\n",
    "        for candidate in candidates:\n",
    "            if candidate.issubset(transaction):\n",
    "                support_counts[candidate] += 1\n",
    "    return support_counts    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c0fe",
   "metadata": {},
   "source": [
    "## Step 3: Generate Association Rules\n",
    "\n",
    "- Use frequent itemsets to generate association rules\n",
    "- For each rule `A => B`, calculate:\n",
    "  - **Support**\n",
    "  - **Confidence**\n",
    "- Only return rules that meet a minimum confidence threshold (e.g., 0.5)\n",
    "\n",
    "ðŸ‘‰ **Implement rule generation function below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate rules from frequent itemsets\n",
    "\n",
    "def generate_rules(frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "\n",
    "    for itemset, support_itemset in frequent_itemsets.items():\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in combinations(itemset, i):\n",
    "                    antecedent = frozenset(antecedent)\n",
    "                    consequent = itemset - antecedent\n",
    "\n",
    "                    support_antecedent = frequent_itemsets.get(antecedent, 0)\n",
    "                    if support_antecedent > 0:\n",
    "                        confidence = support_itemset / support_antecedent\n",
    "                        if confidence >= min_confidence:\n",
    "                            support_consequent = frequent_itemsets.get(consequent, 0)\n",
    "                            lift = confidence / support_consequent if support_consequent > 0 else float('inf')\n",
    "\n",
    "                            rules.append({\n",
    "                                'antecedents': antecedent,\n",
    "                                'consequents': consequent,\n",
    "                                'support': support_itemset,\n",
    "                                'confidence': confidence,\n",
    "                                'lift': lift\n",
    "                            })\n",
    "\n",
    "    return rules\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf26889",
   "metadata": {},
   "source": [
    "## Step 4: Output and Visualize\n",
    "\n",
    "- Print top 10 frequent itemsets\n",
    "- Print top 10 association rules (by confidence or lift)\n",
    "\n",
    "ðŸ‘‰ **Output results below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3443a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Run the algorithm\n",
    "min_support = 0.01 \n",
    "min_confidence = 0.5 \n",
    "\n",
    "frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
    "print(f\"Frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "\n",
    "rules = generate_rules(frequent_itemsets, min_confidence)\n",
    "print(f\"Association rules generated: {len(rules)}\")\n",
    "\n",
    "# Step 6: Output results\n",
    "print(\"\\n## Top 10 Frequent Itemsets (by support)\")\n",
    "sorted_frequent_itemsets = sorted(frequent_itemsets.items(), key=lambda item: item[1], reverse=True)\n",
    "for itemset, support in sorted_frequent_itemsets[:10]:\n",
    "    print(f\"Itemset: {list(itemset)}, Support: {support:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"## Top 10 Association Rules (by lift)\")\n",
    "sorted_rules = sorted(rules, key=lambda rule: rule['lift'], reverse=True)\n",
    "for rule in sorted_rules[:10]:\n",
    "    print(f\"Rule: {list(rule['antecedents'])} => {list(rule['consequents'])}\")\n",
    "    print(f\"  Support: {rule['support']:.4f}, Confidence: {rule['confidence']:.4f}, Lift: {rule['lift']:.4f}\")\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4a1ae-11b9-4d8c-8124-f14930608950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
